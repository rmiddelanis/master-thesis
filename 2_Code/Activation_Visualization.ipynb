{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of Layer Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import data_generator\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.gridspec as gridspec\n",
    "from utils import get_batch, make_experiment_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pgf = True\n",
    "# dataset_selection = \"B\"\n",
    "dataset_selection = \"A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 12})\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "\n",
    "if use_pgf:\n",
    "    matplotlib.use(\"pgf\")\n",
    "    pgf_with_rc_fonts = {\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [],                   # use latex default serif font\n",
    "        \"font.sans-serif\": [\"DejaVu Sans\"], # use a specific sans-serif font\n",
    "    }\n",
    "    matplotlib.rcParams.update(pgf_with_rc_fonts)\n",
    "    matplotlib.rcParams.update({'pgf.rcfonts': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_selection == \"B\":\n",
    "    model_path = \"./Activation_Visualization/BnA__activation_visu__models/BnA/Iteration_0/0_base_B/model.pt\"\n",
    "elif dataset_selection == \"A\":\n",
    "    model_path = \"./Activation_Visualization/AnB__activation_visu__models/BnA/Iteration_0/0_base_B/model.pt\"\n",
    "    \n",
    "experiments_path = \"./Activation_Visualization/Experiments/\"\n",
    "inputs = {}\n",
    "activations = {}\n",
    "forward_hooks = []\n",
    "input_sequences = []\n",
    "input_sequence_name_mapping = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = {\n",
    "    'test_size': 0.2,\n",
    "    'train_valid_split': [1, 0],\n",
    "    'sampling_size': 1,\n",
    "    # 'data_file_name': r'./Transferability_Investigation/synthetic_data.p',\n",
    "    'data_file_name': r'./data/Synthetic/synthetic_data.p',\n",
    "    # 'data_file_name': r'./data/Synthetic/synthetic_data_reverse.p',\n",
    "    # 'data_file_name': r'./data/household_power_consumption.p',\n",
    "    # 'data_file_name': r'./data/household_power_consumption_diff.p',\n",
    "    'sample_frequency': 1,\n",
    "    'max_data': 9999999,\n",
    "    'target_variables': ['Soll'],\n",
    "    # 'target_variables': ['Global_active_power [Soll]', 'Global_reactive_power [Soll]', 'Voltage [Soll]',\n",
    "    #                      'Global_intensity [Soll]', 'Sub_metering_1 [Soll]', 'Sub_metering_2 [Soll]',\n",
    "    #                      'Sub_metering_3 [Soll]'],\n",
    "    'seed': 1,\n",
    "    'series_x': '11',\n",
    "    'series_y': '11',\n",
    "    'pca': False,\n",
    "    'batch_size': 1,\n",
    "    'cuda': True,\n",
    "}\n",
    "\n",
    "if dataset_selection == \"B\":\n",
    "    dataset_config['data_file_name'] = r'./data/Synthetic/synthetic_data.p'\n",
    "elif dataset_selection == \"A\":\n",
    "    dataset_config['data_file_name'] = r'./data/Synthetic/synthetic_data_reverse.p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to register hooks behind ReLU and Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_forward_hook_function(name):\n",
    "    activations[name] = []\n",
    "    inputs[name] = []\n",
    "    def hook_function(module, input, output):\n",
    "        activations[name].append(output[0].data.numpy())\n",
    "        inputs[name].append(input[0].data.numpy())\n",
    "    hook_function.__name__ = \"hook_\"+name\n",
    "    forward_hooks.append(hook_function)\n",
    "    return hook_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_hooks(model):\n",
    "    for resBlock_idx in range(len(model.tcn.network)):\n",
    "        hook_func_relu_1 = add_forward_hook_function(\"relu_\"+str(resBlock_idx)+\"_1\")\n",
    "        hook_func_relu_2 = add_forward_hook_function(\"relu_\"+str(resBlock_idx)+\"_2\")\n",
    "        hook_func_out = add_forward_hook_function(\"out_\"+str(resBlock_idx))\n",
    "        model.tcn.network[resBlock_idx].relu1.register_forward_hook(hook_func_relu_1)\n",
    "        model.tcn.network[resBlock_idx].relu2.register_forward_hook(hook_func_relu_2)\n",
    "        model.tcn.network[resBlock_idx].register_forward_hook(hook_func_out)\n",
    "        # model.tcn.network[resBlock_idx].conv1.register_forward_hook(hook_func1)\n",
    "        # model.tcn.network[resBlock_idx].conv2.register_forward_hook(hook_func2)\n",
    "        # model.tcn.network[resBlock_idx].conv1.bias.data.fill_(0)\n",
    "        # model.tcn.network[resBlock_idx].conv2.bias.data.fill_(0)\n",
    "        # if model.tcn.network[resBlock_idx].downsample is not None:\n",
    "        #     model.tcn.network[resBlock_idx].downsample.bias.data.fill_(0)\n",
    "        if model.tcn.network[resBlock_idx].downsample is not None:\n",
    "            hook_func_downsample = add_forward_hook_function(\"downsample_\" + str(resBlock_idx))\n",
    "            model.tcn.network[resBlock_idx].downsample.register_forward_hook(hook_func_downsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Activation Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layer_activations(output_path=None):\n",
    "    x = np.arange(num_resblocks)\n",
    "    x_ticks = []\n",
    "    y = []\n",
    "    for input_sequence_idx in range(len(input_sequences)):\n",
    "        output_activation = activations[\"out_\"+str(num_resblocks-1)][input_sequence_idx].sum()\n",
    "        input_activation = torch.sum(input_sequences[input_sequence_idx])*50\n",
    "        layer_activations = []\n",
    "        for resblock in range(num_resblocks):\n",
    "            layer_activations.append(activations[\"relu_\"+str(resblock)+\"_2\"][input_sequence_idx].sum() / (output_activation-input_activation))\n",
    "            x_ticks.append(\"Residual Block\"+str(resblock+1))\n",
    "        y.append(layer_activations)\n",
    "    y = np.array(y)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6.8))\n",
    "    for input in range(y.shape[0]):\n",
    "        ax.plot(y[input], label=input_sequence_name_mapping[input])\n",
    "    # ax.plot(np.mean(y, axis=0), linewidth=4, label=\"Mean\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(x_ticks)\n",
    "    ax.set_ylabel(\"Module Influence\")\n",
    "    ax.set_xlabel(\"Network Module\")\n",
    "    ax.grid()\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if use_pgf:\n",
    "        fig.savefig(os.path.join(output_path, \"module_influence.pgf\"), transparency=True)\n",
    "    else:\n",
    "        fig.savefig(os.path.join(output_path, \"module_influence.svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Block Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_res_block_activations(input_sequence_idx, num_columns=10, neurons_to_plot=None, sequence_clip=50, plot_path=None):\n",
    "    if neurons_to_plot is None:\n",
    "        neurons_to_plot = [range(num_neurons[2*resblock + 1]) for resblock in range(num_resblocks)]\n",
    "    elif type(neurons_to_plot) is int:\n",
    "        neurons_to_plot = [range(neurons_to_plot) for resblock in range(num_resblocks)]\n",
    "    elif hasattr(neurons_to_plot, '__iter__'):\n",
    "        if len(neurons_to_plot) != num_resblocks:\n",
    "            raise ValueError('List neurons_to_plot must be of length num_resblocks!')\n",
    "    else:\n",
    "        raise ValueError(\"Parameter neurons_to_plot must be either None or int or iterable\")\n",
    "\n",
    "    # num_rows = int(np.sum([len(neurons_to_plot[resblock]) for resblock in range(num_resblocks)] + len(neurons_to_plot[-1])) / num_columns)\n",
    "    num_rows = int(np.sum([len(neurons_to_plot[resblock]) for resblock in range(num_resblocks)]) / num_columns)\n",
    "    # height_ratios = [2] + [1] * (num_rows + num_resblocks)\n",
    "    height_ratios = ([1.5] + [1] + ([1] * int(num_rows/num_resblocks) + [0.5]) * num_resblocks)[:-1]\n",
    "    # width_ratios = [1/7 * num_columns] + [1] * num_columns\n",
    "    width_ratios = [0.6] + [1] * num_columns\n",
    "    \n",
    "    print(\"Num rows: {}, Num cols: {}\".format(num_rows, num_columns))\n",
    "    print(\"Height_ratios: \", height_ratios)\n",
    "    \n",
    "    figure, axs = plt.subplots(num_rows + 1 + num_resblocks, num_columns + 1, sharex=True, sharey=True,\n",
    "                               gridspec_kw={'height_ratios': height_ratios, 'width_ratios': width_ratios},\n",
    "                               figsize=(20, 13))\n",
    "    gs = axs[0, 0].get_gridspec()\n",
    "    for ax in axs[0, :]:\n",
    "        ax.remove()\n",
    "    for resblock in range(num_resblocks):\n",
    "        for ax in axs[1 + int(resblock*(num_rows/num_resblocks+1)), :]:\n",
    "            ax.remove()\n",
    "\n",
    "    input_sequence = input_sequences[input_sequence_idx][0, 0, :].numpy()\n",
    "    input_len = len(input_sequence)\n",
    "\n",
    "    input_ax = figure.add_subplot(gs[0, 1:])\n",
    "    input_ax.plot(input_sequence, 'ro', c='r', ms=2)\n",
    "    label_ax_inp = figure.add_subplot(gs[0, 0])\n",
    "    label_ax_inp.axis('off')\n",
    "    # label_ax_inp.set_ylim((0, 1))\n",
    "    # label_ax_inp.set_xlim((0, 1))\n",
    "    label_ax_inp.text(0, 0.5, \"Input\")\n",
    "\n",
    "    neuron_counter = 0\n",
    "    for resblock in range(num_resblocks):\n",
    "        start_row = 2 + int(resblock * (num_rows / num_resblocks + 1))\n",
    "        end_row = start_row + int(len(neurons_to_plot[resblock]) / num_columns)\n",
    "        for ax in axs[start_row:end_row, 0]:\n",
    "            ax.remove()\n",
    "        label_ax_resblock = figure.add_subplot(gs[start_row:end_row, 0])\n",
    "        label_ax_resblock.axis('off')\n",
    "        # label_ax_resblock.set_ylim((0, 1))\n",
    "        # label_ax_resblock.set_xlim((0, 1))\n",
    "        label_ax_resblock.text(0, 0.5, \"Block {}\".format(resblock+1), fontsize = 14)\n",
    "        for idx, neuron_idx in enumerate(neurons_to_plot[resblock]):\n",
    "            row_coordiante = int(neuron_counter / num_columns) + resblock + 2\n",
    "            col_coordiante = 1 + (neuron_counter) % num_columns\n",
    "            activation_abs_sum = np.sum(\n",
    "                activations[\"relu_\"+str(resblock)+\"_2\"][input_sequence_idx][neurons_to_plot[resblock][idx], :])\n",
    "            if activation_abs_sum != 0:\n",
    "                axs[row_coordiante, col_coordiante].plot(\n",
    "                    activations[\"relu_\"+str(resblock)+\"_2\"][input_sequence_idx][neurons_to_plot[resblock][idx], :],\n",
    "                    linewidth=1)\n",
    "            else:\n",
    "                axs[row_coordiante, col_coordiante].set_facecolor('darkgray')\n",
    "            axs[row_coordiante, col_coordiante].text(\n",
    "                0.5, 0.78, '{0:.2f}'.format(activation_abs_sum), horizontalalignment='center',\n",
    "                verticalalignment='center', transform=axs[row_coordiante, col_coordiante].transAxes, fontsize = 9)\n",
    "            if col_coordiante == 1:\n",
    "                axs[row_coordiante, col_coordiante].yaxis.set_tick_params(which='both', labelleft=True)\n",
    "            neuron_counter = neuron_counter + 1\n",
    "\n",
    "    if sequence_clip > 0:\n",
    "        axs[2,1].set_xlim(0, sequence_clip-1)\n",
    "    elif sequence_clip < 0:\n",
    "        axs[2, 1].set_xlim(input_len+sequence_clip-1, input_len-1)\n",
    "        \n",
    "    #plt.tight_layout()\n",
    "\n",
    "    if plot_path is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        if use_pgf:\n",
    "            figure.savefig(os.path.join(plot_path, \"activation_maps__\"+input_sequence_name_mapping[input_sequence_idx].replace(' ', '_')+\".pgf\"), transparency=True, bbox_inches='tight')\n",
    "        else:\n",
    "            figure.savefig(os.path.join(plot_path, \"activation_maps__\"+input_sequence_name_mapping[input_sequence_idx].replace(' ', '_')+\".svg\"), format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Neuron Activation over Network Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_relative_neuron_activation(input_sequence_idx=0, output_path=None, num_neurons_to_plot=None):\n",
    "    x = np.arange(num_resblocks + 3)\n",
    "    x_ticks = []\n",
    "    y = []\n",
    "    \n",
    "    plot_neurons = np.arange(num_neurons[0])\n",
    "    if num_neurons_to_plot is not None:\n",
    "        plot_neurons = get_max_activated_channel_manipulations(idx)[:, :num_neurons_to_plot, 0].astype(int)\n",
    "    labels = [\"Channel {}\".format(i+1) for i in plot_neurons]\n",
    "        \n",
    "    for neuron_idx in range(num_neurons[0]):\n",
    "        neuron_activations = []\n",
    "        neuron_activations.append(1)\n",
    "        neuron_activations.append(abs(activations['downsample_0'][input_sequence_idx][neuron_idx].sum()))\n",
    "        # print(neuron_activations[-1])\n",
    "        for resblock in range(num_resblocks):\n",
    "            neuron_activations.append(activations['out_'+str(resblock)][input_sequence_idx][neuron_idx].sum())\n",
    "        neuron_activations.append(abs(activations['out_3'][input_sequence_idx][neuron_idx].sum()*model.linear.weight[0, neuron_idx].item()))\n",
    "        y.append(neuron_activations)\n",
    "    y = np.array(y)\n",
    "\n",
    "    for col_idx in range(y.shape[1]):\n",
    "        y[:, col_idx] = y[:, col_idx] / np.sum(y[:, col_idx])\n",
    "\n",
    "    x_ticks.append(\"__\")\n",
    "    x_ticks.append(\"Init\")\n",
    "    x_ticks.append(\"Conv1D Downsample\")\n",
    "    for resblock in range(num_resblocks):\n",
    "        x_ticks.append(\"ResBlock {}\".format(resblock))\n",
    "    x_ticks.append(\"Linear Layer\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,9))\n",
    "    ax.stackplot(x, y, labels=labels)\n",
    "    ax.set_xticklabels(x_ticks)\n",
    "    ax.set_title(\"Channel Activation Plot: {}\".format(input_sequence_name_mapping[input_sequence_idx]))\n",
    "    ax.set_xlabel(\"Network Module\")\n",
    "    ax.set_ylabel(\"Fraction of the total Activation\")\n",
    "    # plt.legend()\n",
    "    if output_path is not None:\n",
    "        if use_pgf:\n",
    "            fig.savefig(os.path.join(output_path, input_sequence_name_mapping[input_sequence_idx])+\".pgf\", transparency=True)\n",
    "        else:\n",
    "            fig.savefig(os.path.join(output_path, input_sequence_name_mapping[input_sequence_idx])+\".svg\", trformat='svg')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of maximally activated Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_activated_channel_manipulations(input_sequence_idx, reverse=True):\n",
    "    result = []\n",
    "    for resblock in range(num_resblocks):\n",
    "        layer_result = []\n",
    "        for neuron_idx in range(num_neurons[resblock]):\n",
    "            layer_result.append([neuron_idx, activations[\"relu_\"+str(resblock)+\"_2\"][input_sequence_idx][neuron_idx].sum()])\n",
    "        layer_result_sorted = sorted(layer_result,key=lambda x: x[1], reverse=reverse)\n",
    "        result.append(layer_result_sorted)\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(open(model_path, 'rb'), map_location='cpu')\n",
    "model.eval()\n",
    "\n",
    "num_resblocks = len(model.tcn.network)\n",
    "num_layers = num_resblocks*2\n",
    "num_neurons = []\n",
    "for resblock in range(num_resblocks):\n",
    "    num_neurons.append(model.tcn.network[resblock].conv1.bias.shape[0])\n",
    "    num_neurons.append(model.tcn.network[resblock].conv2.bias.shape[0])\n",
    "    \n",
    "register_hooks(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate input sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set params signal length and period length for the signal generation\n",
    "signal_len = 200\n",
    "period_len = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, x_labels, y_labels, pca_scaler = data_generator(dataset_config)\n",
    "\n",
    "x_train = data.x_train\n",
    "x_test = data.x_test\n",
    "x_tune = data.x_valid[0]\n",
    "y_train = data.y_train\n",
    "y_test = data.y_test\n",
    "y_tune = data.y_valid[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences.append(torch.zeros((1, 1, signal_len)).index_fill(2, torch.LongTensor([4]), 1))\n",
    "input_sequence_name_mapping[len(input_sequences)-1] = 'Dirac Function'\n",
    "\n",
    "input_sequences.append(torch.Tensor([((i) % period_len) / period_len for i in range(signal_len)]).reshape((1, 1, signal_len)))\n",
    "input_sequence_name_mapping[len(input_sequences) - 1] = 'Sawtooth'\n",
    "\n",
    "input_sequences.append(torch.Tensor(np.sin(np.arange(signal_len) * 2 * np.pi / period_len) + 1).reshape((1, 1, signal_len)))\n",
    "input_sequence_name_mapping[len(input_sequences) - 1] = 'Sine'\n",
    "\n",
    "input_sequences.append(torch.Tensor([1 if i % period_len < (period_len / 2) else 0 for i in range(signal_len)]).reshape((1, 1, signal_len)))\n",
    "input_sequence_name_mapping[len(input_sequences) - 1] = 'Square'\n",
    "\n",
    "input_sequences.append(torch.zeros((1, 1, signal_len)).fill_(1))\n",
    "input_sequence_name_mapping[len(input_sequences) - 1] = 'Constant'\n",
    "\n",
    "input_sequences.append(torch.Tensor(np.arange(0, signal_len / 10, 0.1)).reshape((1, 1, signal_len)))\n",
    "input_sequence_name_mapping[len(input_sequences) - 1] = 'Linear'\n",
    "\n",
    "input_sequences.append(torch.Tensor([abs(((i) % period_len) / period_len - 0.5) for i in range(signal_len)]).reshape((1, 1, signal_len)))\n",
    "input_sequence_name_mapping[len(input_sequences) - 1] = 'Triangular'\n",
    "\n",
    "input_sequences.append(get_batch(x_train, y_train, int(np.random.rand() * (x_train.shape[2] - signal_len)), signal_len)[0][0:1, 1:2, :])\n",
    "input_sequence_name_mapping[len(input_sequences) - 1] = 'Training Sequence'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed sequences into the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(input_sequences)):\n",
    "    model(input_sequences[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create experiment folder \n",
    "# experiment_dir = make_experiment_dir(experiments_path)\n",
    "experiment_dir = './Activation_Visualization/Experiments/2019-07-19 15-01-48.156239__A__final/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResBlock Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set params for clipping the output sequences and set number of columns in the plot\n",
    "sequence_clip = 50\n",
    "num_columns = 10\n",
    "\n",
    "# only plot the num_neurons_to_plot <= 50 neurons with the highest activation\n",
    "num_neurons_to_plot = 1\n",
    "neurons_to_plot = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num rows: 20, Num cols: 10\n",
      "Height_ratios:  [1.5, 1, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1]\n",
      "Num rows: 20, Num cols: 10\n",
      "Height_ratios:  [1.5, 1, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1]\n",
      "Num rows: 20, Num cols: 10\n",
      "Height_ratios:  [1.5, 1, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1]\n",
      "Num rows: 20, Num cols: 10\n",
      "Height_ratios:  [1.5, 1, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1]\n",
      "Num rows: 20, Num cols: 10\n",
      "Height_ratios:  [1.5, 1, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1]\n",
      "Num rows: 20, Num cols: 10\n",
      "Height_ratios:  [1.5, 1, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1]\n",
      "Num rows: 20, Num cols: 10\n",
      "Height_ratios:  [1.5, 1, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1]\n",
      "Num rows: 20, Num cols: 10\n",
      "Height_ratios:  [1.5, 1, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1, 0.5, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(input_sequences)):\n",
    "    #neurons_to_plot = get_max_activated_channel_manipulations(idx)[:, :num_neurons_to_plot, 0].astype(int)\n",
    "    plot_res_block_activations(input_sequence_idx=idx, num_columns=num_columns, neurons_to_plot=neurons_to_plot, sequence_clip=sequence_clip, plot_path=experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\pytorchtutorial\\lib\\site-packages\\matplotlib\\figure.py:445: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  % get_backend())\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(input_sequences)):\n",
    "    plot_relative_neuron_activation(input_sequence_idx=idx, output_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_layer_activations(experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting batch nr. 0 out of 1800\n",
      "Predicting batch nr. 10 out of 1800\n",
      "Predicting batch nr. 20 out of 1800\n",
      "Predicting batch nr. 30 out of 1800\n",
      "Predicting batch nr. 40 out of 1800\n",
      "Predicting batch nr. 50 out of 1800\n",
      "Predicting batch nr. 60 out of 1800\n",
      "Predicting batch nr. 70 out of 1800\n",
      "Predicting batch nr. 80 out of 1800\n",
      "Predicting batch nr. 90 out of 1800\n",
      "Predicting batch nr. 100 out of 1800\n",
      "Predicting batch nr. 110 out of 1800\n",
      "Predicting batch nr. 120 out of 1800\n",
      "Predicting batch nr. 130 out of 1800\n",
      "Predicting batch nr. 140 out of 1800\n",
      "Predicting batch nr. 150 out of 1800\n",
      "Predicting batch nr. 160 out of 1800\n",
      "Predicting batch nr. 170 out of 1800\n",
      "Predicting batch nr. 180 out of 1800\n",
      "Predicting batch nr. 190 out of 1800\n",
      "Predicting batch nr. 200 out of 1800\n",
      "Predicting batch nr. 210 out of 1800\n",
      "Predicting batch nr. 220 out of 1800\n",
      "Predicting batch nr. 230 out of 1800\n",
      "Predicting batch nr. 240 out of 1800\n",
      "Predicting batch nr. 250 out of 1800\n",
      "Predicting batch nr. 260 out of 1800\n",
      "Predicting batch nr. 270 out of 1800\n",
      "Predicting batch nr. 280 out of 1800\n",
      "Predicting batch nr. 290 out of 1800\n",
      "Predicting batch nr. 300 out of 1800\n",
      "Predicting batch nr. 310 out of 1800\n",
      "Predicting batch nr. 320 out of 1800\n",
      "Predicting batch nr. 330 out of 1800\n",
      "Predicting batch nr. 340 out of 1800\n",
      "Predicting batch nr. 350 out of 1800\n",
      "Predicting batch nr. 360 out of 1800\n",
      "Predicting batch nr. 370 out of 1800\n",
      "Predicting batch nr. 380 out of 1800\n",
      "Predicting batch nr. 390 out of 1800\n",
      "Predicting batch nr. 400 out of 1800\n",
      "Predicting batch nr. 410 out of 1800\n",
      "Predicting batch nr. 420 out of 1800\n",
      "Predicting batch nr. 430 out of 1800\n",
      "Predicting batch nr. 440 out of 1800\n",
      "Predicting batch nr. 450 out of 1800\n",
      "Predicting batch nr. 460 out of 1800\n",
      "Predicting batch nr. 470 out of 1800\n",
      "Predicting batch nr. 480 out of 1800\n",
      "Predicting batch nr. 490 out of 1800\n",
      "Predicting batch nr. 500 out of 1800\n",
      "Predicting batch nr. 510 out of 1800\n",
      "Predicting batch nr. 520 out of 1800\n",
      "Predicting batch nr. 530 out of 1800\n",
      "Predicting batch nr. 540 out of 1800\n",
      "Predicting batch nr. 550 out of 1800\n",
      "Predicting batch nr. 560 out of 1800\n",
      "Predicting batch nr. 570 out of 1800\n",
      "Predicting batch nr. 580 out of 1800\n",
      "Predicting batch nr. 590 out of 1800\n",
      "Predicting batch nr. 600 out of 1800\n",
      "Predicting batch nr. 610 out of 1800\n",
      "Predicting batch nr. 620 out of 1800\n",
      "Predicting batch nr. 630 out of 1800\n",
      "Predicting batch nr. 640 out of 1800\n",
      "Predicting batch nr. 650 out of 1800\n",
      "Predicting batch nr. 660 out of 1800\n",
      "Predicting batch nr. 670 out of 1800\n",
      "Predicting batch nr. 680 out of 1800\n",
      "Predicting batch nr. 690 out of 1800\n",
      "Predicting batch nr. 700 out of 1800\n",
      "Predicting batch nr. 710 out of 1800\n",
      "Predicting batch nr. 720 out of 1800\n",
      "Predicting batch nr. 730 out of 1800\n",
      "Predicting batch nr. 740 out of 1800\n",
      "Predicting batch nr. 750 out of 1800\n",
      "Predicting batch nr. 760 out of 1800\n",
      "Predicting batch nr. 770 out of 1800\n",
      "Predicting batch nr. 780 out of 1800\n",
      "Predicting batch nr. 790 out of 1800\n",
      "Predicting batch nr. 800 out of 1800\n",
      "Predicting batch nr. 810 out of 1800\n",
      "Predicting batch nr. 820 out of 1800\n",
      "Predicting batch nr. 830 out of 1800\n",
      "Predicting batch nr. 840 out of 1800\n",
      "Predicting batch nr. 850 out of 1800\n",
      "Predicting batch nr. 860 out of 1800\n",
      "Predicting batch nr. 870 out of 1800\n",
      "Predicting batch nr. 880 out of 1800\n",
      "Predicting batch nr. 890 out of 1800\n",
      "Predicting batch nr. 900 out of 1800\n",
      "Predicting batch nr. 910 out of 1800\n",
      "Predicting batch nr. 920 out of 1800\n",
      "Predicting batch nr. 930 out of 1800\n",
      "Predicting batch nr. 940 out of 1800\n",
      "Predicting batch nr. 950 out of 1800\n",
      "Predicting batch nr. 960 out of 1800\n",
      "Predicting batch nr. 970 out of 1800\n",
      "Predicting batch nr. 980 out of 1800\n",
      "Predicting batch nr. 990 out of 1800\n",
      "Predicting batch nr. 1000 out of 1800\n",
      "Predicting batch nr. 1010 out of 1800\n",
      "Predicting batch nr. 1020 out of 1800\n",
      "Predicting batch nr. 1030 out of 1800\n",
      "Predicting batch nr. 1040 out of 1800\n",
      "Predicting batch nr. 1050 out of 1800\n",
      "Predicting batch nr. 1060 out of 1800\n",
      "Predicting batch nr. 1070 out of 1800\n",
      "Predicting batch nr. 1080 out of 1800\n",
      "Predicting batch nr. 1090 out of 1800\n",
      "Predicting batch nr. 1100 out of 1800\n",
      "Predicting batch nr. 1110 out of 1800\n",
      "Predicting batch nr. 1120 out of 1800\n",
      "Predicting batch nr. 1130 out of 1800\n",
      "Predicting batch nr. 1140 out of 1800\n",
      "Predicting batch nr. 1150 out of 1800\n",
      "Predicting batch nr. 1160 out of 1800\n",
      "Predicting batch nr. 1170 out of 1800\n",
      "Predicting batch nr. 1180 out of 1800\n",
      "Predicting batch nr. 1190 out of 1800\n",
      "Predicting batch nr. 1200 out of 1800\n",
      "Predicting batch nr. 1210 out of 1800\n",
      "Predicting batch nr. 1220 out of 1800\n",
      "Predicting batch nr. 1230 out of 1800\n",
      "Predicting batch nr. 1240 out of 1800\n",
      "Predicting batch nr. 1250 out of 1800\n",
      "Predicting batch nr. 1260 out of 1800\n",
      "Predicting batch nr. 1270 out of 1800\n",
      "Predicting batch nr. 1280 out of 1800\n",
      "Predicting batch nr. 1290 out of 1800\n",
      "Predicting batch nr. 1300 out of 1800\n",
      "Predicting batch nr. 1310 out of 1800\n",
      "Predicting batch nr. 1320 out of 1800\n",
      "Predicting batch nr. 1330 out of 1800\n",
      "Predicting batch nr. 1340 out of 1800\n",
      "Predicting batch nr. 1350 out of 1800\n",
      "Predicting batch nr. 1360 out of 1800\n",
      "Predicting batch nr. 1370 out of 1800\n",
      "Predicting batch nr. 1380 out of 1800\n",
      "Predicting batch nr. 1390 out of 1800\n",
      "Predicting batch nr. 1400 out of 1800\n",
      "Predicting batch nr. 1410 out of 1800\n",
      "Predicting batch nr. 1420 out of 1800\n",
      "Predicting batch nr. 1430 out of 1800\n",
      "Predicting batch nr. 1440 out of 1800\n",
      "Predicting batch nr. 1450 out of 1800\n",
      "Predicting batch nr. 1460 out of 1800\n",
      "Predicting batch nr. 1470 out of 1800\n",
      "Predicting batch nr. 1480 out of 1800\n",
      "Predicting batch nr. 1490 out of 1800\n",
      "Predicting batch nr. 1500 out of 1800\n",
      "Predicting batch nr. 1510 out of 1800\n",
      "Predicting batch nr. 1520 out of 1800\n",
      "Predicting batch nr. 1530 out of 1800\n",
      "Predicting batch nr. 1540 out of 1800\n",
      "Predicting batch nr. 1550 out of 1800\n",
      "Predicting batch nr. 1560 out of 1800\n",
      "Predicting batch nr. 1570 out of 1800\n",
      "Predicting batch nr. 1580 out of 1800\n",
      "Predicting batch nr. 1590 out of 1800\n",
      "Predicting batch nr. 1600 out of 1800\n",
      "Predicting batch nr. 1610 out of 1800\n",
      "Predicting batch nr. 1620 out of 1800\n",
      "Predicting batch nr. 1630 out of 1800\n",
      "Predicting batch nr. 1640 out of 1800\n",
      "Predicting batch nr. 1650 out of 1800\n",
      "Predicting batch nr. 1660 out of 1800\n",
      "Predicting batch nr. 1670 out of 1800\n",
      "Predicting batch nr. 1680 out of 1800\n",
      "Predicting batch nr. 1690 out of 1800\n",
      "Predicting batch nr. 1700 out of 1800\n",
      "Predicting batch nr. 1710 out of 1800\n",
      "Predicting batch nr. 1720 out of 1800\n",
      "Predicting batch nr. 1730 out of 1800\n",
      "Predicting batch nr. 1740 out of 1800\n",
      "Predicting batch nr. 1750 out of 1800\n",
      "Predicting batch nr. 1760 out of 1800\n",
      "Predicting batch nr. 1770 out of 1800\n",
      "Predicting batch nr. 1780 out of 1800\n",
      "Predicting batch nr. 1790 out of 1800\n"
     ]
    }
   ],
   "source": [
    "for key in list(activations.keys()):\n",
    "    inputs[key] = []\n",
    "    activations[key] = []\n",
    "input_sequences = []\n",
    "\n",
    "for batch_idx in range(x_test.shape[2] - signal_len):\n",
    "#for batch_idx in range(500):\n",
    "    if batch_idx % 10 == 0:\n",
    "        print(f\"Predicting batch nr. {batch_idx} out of {x_test.shape[2] - signal_len}\")\n",
    "    batch = get_batch(x_test, y_test, batch_idx, signal_len)[0][0:1, 1:2, :]\n",
    "    model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_sums = [np.zeros(50)]*4\n",
    "\n",
    "activation_means = {}\n",
    "for module_name in ['relu_0_2', 'relu_1_2', 'relu_2_2', 'relu_3_2']:\n",
    "    activation_means[module_name] = np.array(activations[module_name]).sum(axis=2).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average activation of all manipulations for ResBlocks 1 through 4:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relu_0_2</th>\n",
       "      <th>relu_1_2</th>\n",
       "      <th>relu_2_2</th>\n",
       "      <th>relu_3_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>492.303009</td>\n",
       "      <td>111.747643</td>\n",
       "      <td>127.880501</td>\n",
       "      <td>181.656265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>353.012512</td>\n",
       "      <td>172.139587</td>\n",
       "      <td>87.859955</td>\n",
       "      <td>132.849289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>149.799637</td>\n",
       "      <td>12.566501</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.105091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.375992</td>\n",
       "      <td>14.659655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.687305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>158.685471</td>\n",
       "      <td>174.721634</td>\n",
       "      <td>229.592102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>268.133636</td>\n",
       "      <td>89.658730</td>\n",
       "      <td>44.153465</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>228.321686</td>\n",
       "      <td>43.974369</td>\n",
       "      <td>4.009449</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>147.253433</td>\n",
       "      <td>133.082718</td>\n",
       "      <td>244.868179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>214.698242</td>\n",
       "      <td>61.678093</td>\n",
       "      <td>63.911331</td>\n",
       "      <td>5.199739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>139.492279</td>\n",
       "      <td>29.947077</td>\n",
       "      <td>64.393631</td>\n",
       "      <td>7.700817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>205.968246</td>\n",
       "      <td>89.079926</td>\n",
       "      <td>184.811157</td>\n",
       "      <td>8.867789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>123.142517</td>\n",
       "      <td>1.162884</td>\n",
       "      <td>5.682608</td>\n",
       "      <td>0.046569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>236.253616</td>\n",
       "      <td>54.770012</td>\n",
       "      <td>20.319996</td>\n",
       "      <td>66.467110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>285.970642</td>\n",
       "      <td>140.188019</td>\n",
       "      <td>199.464630</td>\n",
       "      <td>291.506653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>252.119568</td>\n",
       "      <td>57.537235</td>\n",
       "      <td>0.215103</td>\n",
       "      <td>2.128735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>231.900970</td>\n",
       "      <td>51.704922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>274.848206</td>\n",
       "      <td>33.782200</td>\n",
       "      <td>7.552365</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>154.982239</td>\n",
       "      <td>8.150429</td>\n",
       "      <td>0.154440</td>\n",
       "      <td>5.169422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>95.067589</td>\n",
       "      <td>7.359729</td>\n",
       "      <td>8.827353</td>\n",
       "      <td>0.008635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>195.186646</td>\n",
       "      <td>58.994965</td>\n",
       "      <td>14.640758</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>107.601624</td>\n",
       "      <td>20.260471</td>\n",
       "      <td>46.171471</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>278.284241</td>\n",
       "      <td>195.177338</td>\n",
       "      <td>2.099226</td>\n",
       "      <td>138.684143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>259.484375</td>\n",
       "      <td>1.273201</td>\n",
       "      <td>3.434111</td>\n",
       "      <td>7.949226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>162.340958</td>\n",
       "      <td>12.043814</td>\n",
       "      <td>24.404781</td>\n",
       "      <td>30.789349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>222.088989</td>\n",
       "      <td>153.667923</td>\n",
       "      <td>164.420822</td>\n",
       "      <td>240.150635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>111.616478</td>\n",
       "      <td>9.324738</td>\n",
       "      <td>18.139736</td>\n",
       "      <td>2.119498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>238.062073</td>\n",
       "      <td>40.478966</td>\n",
       "      <td>0.084251</td>\n",
       "      <td>21.530846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>104.613724</td>\n",
       "      <td>13.034392</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>105.225502</td>\n",
       "      <td>16.372120</td>\n",
       "      <td>6.823229</td>\n",
       "      <td>1.523542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>259.906189</td>\n",
       "      <td>6.495318</td>\n",
       "      <td>15.335156</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>241.749588</td>\n",
       "      <td>1.581213</td>\n",
       "      <td>14.466519</td>\n",
       "      <td>11.791686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>137.193405</td>\n",
       "      <td>223.404388</td>\n",
       "      <td>155.016678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>221.743668</td>\n",
       "      <td>117.960732</td>\n",
       "      <td>49.705570</td>\n",
       "      <td>116.644386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>168.943024</td>\n",
       "      <td>41.926147</td>\n",
       "      <td>41.142384</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>310.587494</td>\n",
       "      <td>93.553352</td>\n",
       "      <td>33.321850</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>288.470276</td>\n",
       "      <td>10.050556</td>\n",
       "      <td>66.402199</td>\n",
       "      <td>0.996154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>265.745056</td>\n",
       "      <td>26.011066</td>\n",
       "      <td>53.702137</td>\n",
       "      <td>9.179419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>231.048050</td>\n",
       "      <td>43.875984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.738323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>382.482300</td>\n",
       "      <td>130.437668</td>\n",
       "      <td>132.755768</td>\n",
       "      <td>119.150574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>134.019623</td>\n",
       "      <td>6.640808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.259351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>500.176056</td>\n",
       "      <td>1.401301</td>\n",
       "      <td>61.390606</td>\n",
       "      <td>0.000303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>144.693405</td>\n",
       "      <td>54.709885</td>\n",
       "      <td>1.104348</td>\n",
       "      <td>0.937763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>181.496262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>226.615723</td>\n",
       "      <td>13.936247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.547691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>184.705566</td>\n",
       "      <td>13.811836</td>\n",
       "      <td>18.956703</td>\n",
       "      <td>16.754950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>382.675903</td>\n",
       "      <td>146.768372</td>\n",
       "      <td>104.251625</td>\n",
       "      <td>87.193909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>262.272217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.408447</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>155.212967</td>\n",
       "      <td>27.135592</td>\n",
       "      <td>17.019289</td>\n",
       "      <td>40.190002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>203.903442</td>\n",
       "      <td>37.213741</td>\n",
       "      <td>14.238226</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>222.029343</td>\n",
       "      <td>18.490513</td>\n",
       "      <td>39.100548</td>\n",
       "      <td>6.203441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      relu_0_2    relu_1_2    relu_2_2    relu_3_2\n",
       "0   492.303009  111.747643  127.880501  181.656265\n",
       "1   353.012512  172.139587   87.859955  132.849289\n",
       "2   149.799637   12.566501    0.000000   22.105091\n",
       "3   100.375992   14.659655    0.000000   20.687305\n",
       "4     0.000000  158.685471  174.721634  229.592102\n",
       "5   268.133636   89.658730   44.153465    0.000000\n",
       "6   228.321686   43.974369    4.009449    0.000000\n",
       "7     0.000000  147.253433  133.082718  244.868179\n",
       "8   214.698242   61.678093   63.911331    5.199739\n",
       "9   139.492279   29.947077   64.393631    7.700817\n",
       "10  205.968246   89.079926  184.811157    8.867789\n",
       "11  123.142517    1.162884    5.682608    0.046569\n",
       "12  236.253616   54.770012   20.319996   66.467110\n",
       "13  285.970642  140.188019  199.464630  291.506653\n",
       "14  252.119568   57.537235    0.215103    2.128735\n",
       "15  231.900970   51.704922    0.000000    0.000000\n",
       "16  274.848206   33.782200    7.552365    0.000000\n",
       "17  154.982239    8.150429    0.154440    5.169422\n",
       "18   95.067589    7.359729    8.827353    0.008635\n",
       "19  195.186646   58.994965   14.640758    0.000000\n",
       "20  107.601624   20.260471   46.171471    0.000000\n",
       "21  278.284241  195.177338    2.099226  138.684143\n",
       "22  259.484375    1.273201    3.434111    7.949226\n",
       "23  162.340958   12.043814   24.404781   30.789349\n",
       "24  222.088989  153.667923  164.420822  240.150635\n",
       "25  111.616478    9.324738   18.139736    2.119498\n",
       "26  238.062073   40.478966    0.084251   21.530846\n",
       "27    0.000000  104.613724   13.034392    0.000000\n",
       "28  105.225502   16.372120    6.823229    1.523542\n",
       "29  259.906189    6.495318   15.335156    0.000000\n",
       "30  241.749588    1.581213   14.466519   11.791686\n",
       "31    0.000000  137.193405  223.404388  155.016678\n",
       "32  221.743668  117.960732   49.705570  116.644386\n",
       "33  168.943024   41.926147   41.142384    0.000000\n",
       "34  310.587494   93.553352   33.321850    0.000000\n",
       "35  288.470276   10.050556   66.402199    0.996154\n",
       "36  265.745056   26.011066   53.702137    9.179419\n",
       "37  231.048050   43.875984    0.000000   43.738323\n",
       "38  382.482300  130.437668  132.755768  119.150574\n",
       "39  134.019623    6.640808    0.000000    1.259351\n",
       "40  500.176056    1.401301   61.390606    0.000303\n",
       "41  144.693405   54.709885    1.104348    0.937763\n",
       "42  181.496262    0.000000    0.000000    0.000000\n",
       "43  226.615723   13.936247    0.000000    8.547691\n",
       "44  184.705566   13.811836   18.956703   16.754950\n",
       "45  382.675903  146.768372  104.251625   87.193909\n",
       "46  262.272217    0.000000    1.408447    0.000000\n",
       "47  155.212967   27.135592   17.019289   40.190002\n",
       "48  203.903442   37.213741   14.238226    0.000000\n",
       "49  222.029343   18.490513   39.100548    6.203441"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_mean_df = pd.DataFrame(activation_means)\n",
    "print(\"The average activation of all manipulations for ResBlocks 1 through 4:\")\n",
    "#for i in activation_mean_df.round(2)['relu_3_2'].values:\n",
    "#    print(i) \n",
    "activation_mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relu_0_2</th>\n",
       "      <th>relu_1_2</th>\n",
       "      <th>relu_2_2</th>\n",
       "      <th>relu_3_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>relu_0_2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.092376</td>\n",
       "      <td>0.037263</td>\n",
       "      <td>0.018511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relu_1_2</th>\n",
       "      <td>0.092376</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.668465</td>\n",
       "      <td>0.789486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relu_2_2</th>\n",
       "      <td>0.037263</td>\n",
       "      <td>0.668465</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.754535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relu_3_2</th>\n",
       "      <td>0.018511</td>\n",
       "      <td>0.789486</td>\n",
       "      <td>0.754535</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          relu_0_2  relu_1_2  relu_2_2  relu_3_2\n",
       "relu_0_2  1.000000  0.092376  0.037263  0.018511\n",
       "relu_1_2  0.092376  1.000000  0.668465  0.789486\n",
       "relu_2_2  0.037263  0.668465  1.000000  0.754535\n",
       "relu_3_2  0.018511  0.789486  0.754535  1.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_mean_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The channels with the highest manipulation activation for ResBlocks 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relu_0_2</th>\n",
       "      <th>relu_1_2</th>\n",
       "      <th>relu_2_2</th>\n",
       "      <th>relu_3_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>500.176056</td>\n",
       "      <td>1.401301</td>\n",
       "      <td>61.390606</td>\n",
       "      <td>0.000303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>492.303009</td>\n",
       "      <td>111.747643</td>\n",
       "      <td>127.880501</td>\n",
       "      <td>181.656265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>382.675903</td>\n",
       "      <td>146.768372</td>\n",
       "      <td>104.251625</td>\n",
       "      <td>87.193909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>382.482300</td>\n",
       "      <td>130.437668</td>\n",
       "      <td>132.755768</td>\n",
       "      <td>119.150574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>353.012512</td>\n",
       "      <td>172.139587</td>\n",
       "      <td>87.859955</td>\n",
       "      <td>132.849289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>310.587494</td>\n",
       "      <td>93.553352</td>\n",
       "      <td>33.321850</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>288.470276</td>\n",
       "      <td>10.050556</td>\n",
       "      <td>66.402199</td>\n",
       "      <td>0.996154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>285.970642</td>\n",
       "      <td>140.188019</td>\n",
       "      <td>199.464630</td>\n",
       "      <td>291.506653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>278.284241</td>\n",
       "      <td>195.177338</td>\n",
       "      <td>2.099226</td>\n",
       "      <td>138.684143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>274.848206</td>\n",
       "      <td>33.782200</td>\n",
       "      <td>7.552365</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      relu_0_2    relu_1_2    relu_2_2    relu_3_2\n",
       "40  500.176056    1.401301   61.390606    0.000303\n",
       "0   492.303009  111.747643  127.880501  181.656265\n",
       "45  382.675903  146.768372  104.251625   87.193909\n",
       "38  382.482300  130.437668  132.755768  119.150574\n",
       "1   353.012512  172.139587   87.859955  132.849289\n",
       "34  310.587494   93.553352   33.321850    0.000000\n",
       "35  288.470276   10.050556   66.402199    0.996154\n",
       "13  285.970642  140.188019  199.464630  291.506653\n",
       "21  278.284241  195.177338    2.099226  138.684143\n",
       "16  274.848206   33.782200    7.552365    0.000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The channels with the highest manipulation activation for ResBlocks 1:\")\n",
    "activation_mean_df.sort_values('relu_0_2', axis=0, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The channels with the highest manipulation activation for ResBlocks 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relu_0_2</th>\n",
       "      <th>relu_1_2</th>\n",
       "      <th>relu_2_2</th>\n",
       "      <th>relu_3_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>278.284241</td>\n",
       "      <td>195.177338</td>\n",
       "      <td>2.099226</td>\n",
       "      <td>138.684143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>353.012512</td>\n",
       "      <td>172.139587</td>\n",
       "      <td>87.859955</td>\n",
       "      <td>132.849289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>158.685471</td>\n",
       "      <td>174.721634</td>\n",
       "      <td>229.592102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>222.088989</td>\n",
       "      <td>153.667923</td>\n",
       "      <td>164.420822</td>\n",
       "      <td>240.150635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>147.253433</td>\n",
       "      <td>133.082718</td>\n",
       "      <td>244.868179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>382.675903</td>\n",
       "      <td>146.768372</td>\n",
       "      <td>104.251625</td>\n",
       "      <td>87.193909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>285.970642</td>\n",
       "      <td>140.188019</td>\n",
       "      <td>199.464630</td>\n",
       "      <td>291.506653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>137.193405</td>\n",
       "      <td>223.404388</td>\n",
       "      <td>155.016678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>382.482300</td>\n",
       "      <td>130.437668</td>\n",
       "      <td>132.755768</td>\n",
       "      <td>119.150574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>221.743668</td>\n",
       "      <td>117.960732</td>\n",
       "      <td>49.705570</td>\n",
       "      <td>116.644386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      relu_0_2    relu_1_2    relu_2_2    relu_3_2\n",
       "21  278.284241  195.177338    2.099226  138.684143\n",
       "1   353.012512  172.139587   87.859955  132.849289\n",
       "4     0.000000  158.685471  174.721634  229.592102\n",
       "24  222.088989  153.667923  164.420822  240.150635\n",
       "7     0.000000  147.253433  133.082718  244.868179\n",
       "45  382.675903  146.768372  104.251625   87.193909\n",
       "13  285.970642  140.188019  199.464630  291.506653\n",
       "31    0.000000  137.193405  223.404388  155.016678\n",
       "38  382.482300  130.437668  132.755768  119.150574\n",
       "32  221.743668  117.960732   49.705570  116.644386"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The channels with the highest manipulation activation for ResBlocks 2:\")\n",
    "activation_mean_df.sort_values('relu_1_2', axis=0, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The channels with the highest manipulation activation for ResBlocks 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relu_0_2</th>\n",
       "      <th>relu_1_2</th>\n",
       "      <th>relu_2_2</th>\n",
       "      <th>relu_3_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>137.193405</td>\n",
       "      <td>223.404388</td>\n",
       "      <td>155.016678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>285.970642</td>\n",
       "      <td>140.188019</td>\n",
       "      <td>199.464630</td>\n",
       "      <td>291.506653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>205.968246</td>\n",
       "      <td>89.079926</td>\n",
       "      <td>184.811157</td>\n",
       "      <td>8.867789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>158.685471</td>\n",
       "      <td>174.721634</td>\n",
       "      <td>229.592102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>222.088989</td>\n",
       "      <td>153.667923</td>\n",
       "      <td>164.420822</td>\n",
       "      <td>240.150635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>147.253433</td>\n",
       "      <td>133.082718</td>\n",
       "      <td>244.868179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>382.482300</td>\n",
       "      <td>130.437668</td>\n",
       "      <td>132.755768</td>\n",
       "      <td>119.150574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>492.303009</td>\n",
       "      <td>111.747643</td>\n",
       "      <td>127.880501</td>\n",
       "      <td>181.656265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>382.675903</td>\n",
       "      <td>146.768372</td>\n",
       "      <td>104.251625</td>\n",
       "      <td>87.193909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>353.012512</td>\n",
       "      <td>172.139587</td>\n",
       "      <td>87.859955</td>\n",
       "      <td>132.849289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      relu_0_2    relu_1_2    relu_2_2    relu_3_2\n",
       "31    0.000000  137.193405  223.404388  155.016678\n",
       "13  285.970642  140.188019  199.464630  291.506653\n",
       "10  205.968246   89.079926  184.811157    8.867789\n",
       "4     0.000000  158.685471  174.721634  229.592102\n",
       "24  222.088989  153.667923  164.420822  240.150635\n",
       "7     0.000000  147.253433  133.082718  244.868179\n",
       "38  382.482300  130.437668  132.755768  119.150574\n",
       "0   492.303009  111.747643  127.880501  181.656265\n",
       "45  382.675903  146.768372  104.251625   87.193909\n",
       "1   353.012512  172.139587   87.859955  132.849289"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The channels with the highest manipulation activation for ResBlocks 3:\")\n",
    "activation_mean_df.sort_values('relu_2_2', axis=0, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The channels with the highest manipulation activation for ResBlocks 4:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relu_0_2</th>\n",
       "      <th>relu_1_2</th>\n",
       "      <th>relu_2_2</th>\n",
       "      <th>relu_3_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>285.970642</td>\n",
       "      <td>140.188019</td>\n",
       "      <td>199.464630</td>\n",
       "      <td>291.506653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>147.253433</td>\n",
       "      <td>133.082718</td>\n",
       "      <td>244.868179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>222.088989</td>\n",
       "      <td>153.667923</td>\n",
       "      <td>164.420822</td>\n",
       "      <td>240.150635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>158.685471</td>\n",
       "      <td>174.721634</td>\n",
       "      <td>229.592102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>492.303009</td>\n",
       "      <td>111.747643</td>\n",
       "      <td>127.880501</td>\n",
       "      <td>181.656265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>137.193405</td>\n",
       "      <td>223.404388</td>\n",
       "      <td>155.016678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>278.284241</td>\n",
       "      <td>195.177338</td>\n",
       "      <td>2.099226</td>\n",
       "      <td>138.684143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>353.012512</td>\n",
       "      <td>172.139587</td>\n",
       "      <td>87.859955</td>\n",
       "      <td>132.849289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>382.482300</td>\n",
       "      <td>130.437668</td>\n",
       "      <td>132.755768</td>\n",
       "      <td>119.150574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>221.743668</td>\n",
       "      <td>117.960732</td>\n",
       "      <td>49.705570</td>\n",
       "      <td>116.644386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      relu_0_2    relu_1_2    relu_2_2    relu_3_2\n",
       "13  285.970642  140.188019  199.464630  291.506653\n",
       "7     0.000000  147.253433  133.082718  244.868179\n",
       "24  222.088989  153.667923  164.420822  240.150635\n",
       "4     0.000000  158.685471  174.721634  229.592102\n",
       "0   492.303009  111.747643  127.880501  181.656265\n",
       "31    0.000000  137.193405  223.404388  155.016678\n",
       "21  278.284241  195.177338    2.099226  138.684143\n",
       "1   353.012512  172.139587   87.859955  132.849289\n",
       "38  382.482300  130.437668  132.755768  119.150574\n",
       "32  221.743668  117.960732   49.705570  116.644386"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The channels with the highest manipulation activation for ResBlocks 4:\")\n",
    "activation_mean_df.sort_values('relu_3_2', axis=0, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 dead neuron(s) in relu_0_2\n",
      "2 dead neuron(s) in relu_1_2\n",
      "7 dead neuron(s) in relu_2_2\n",
      "13 dead neuron(s) in relu_3_2\n"
     ]
    }
   ],
   "source": [
    "for col in activation_mean_df.columns:\n",
    "    print(f'{activation_mean_df[activation_mean_df[col]==0].shape[0]} dead neuron(s) in {col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_mean_cum_df = pd.DataFrame()\n",
    "activation_mean_cum_df['relu_0_2'] = activation_mean_df['relu_0_2']\n",
    "for block in range(1,4):\n",
    "    activation_mean_cum_df['relu_'+str(block)+'_2'] = activation_mean_cum_df['relu_'+str(block-1)+'_2'] + activation_mean_df['relu_'+str(block)+'_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Total Activation')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_mean_cum_df\n",
    "x = activation_mean_df.columns\n",
    "y = activation_mean_df.values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,15))\n",
    "ax.stackplot(x, y)\n",
    "#ax.set_xticklabels(x_ticks)\n",
    "#ax.set_title(\"Channel Activation Plot: {}\".format(input_sequence_name_mapping[input_sequence_idx]))\n",
    "ax.set_xlabel(\"Network Module\")\n",
    "ax.set_ylabel(\"Total Activation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.7096603e+01, 4.2919971e+01, 1.2004096e+01, 7.1927881e+00,\n",
       "       5.1311390e+01, 0.0000000e+00, 0.0000000e+00, 5.0395615e+01,\n",
       "       3.2524462e+00, 9.8824539e+00, 2.6324799e+00, 8.1413887e-02,\n",
       "       1.6931885e+01, 7.5089684e+01, 3.1778183e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 1.7011354e+00, 2.8357171e-02, 0.0000000e+00,\n",
       "       0.0000000e+00, 3.9822254e+01, 6.6588593e+00, 6.2066932e+00,\n",
       "       6.1474373e+01, 2.5121663e+00, 5.2011957e+00, 0.0000000e+00,\n",
       "       2.4294126e+00, 0.0000000e+00, 6.4955544e+00, 5.8914665e+01,\n",
       "       2.6994286e+01, 0.0000000e+00, 0.0000000e+00, 5.7051313e-01,\n",
       "       4.1525860e+00, 2.0616915e+01, 1.6706116e+01, 2.0507543e+00,\n",
       "       1.5261322e-03, 1.8715228e+00, 0.0000000e+00, 1.5115102e+01,\n",
       "       2.6871278e+00, 2.2702076e+01, 0.0000000e+00, 1.9646498e+01,\n",
       "       0.0000000e+00, 6.4080958e+00], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(np.array(activations['relu_3_2']).sum(axis=2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.8165627e+02, 1.3284929e+02, 2.2105091e+01, 2.0687305e+01,\n",
       "       2.2959210e+02, 0.0000000e+00, 0.0000000e+00, 2.4486818e+02,\n",
       "       5.1997395e+00, 7.7008171e+00, 8.8677893e+00, 4.6569478e-02,\n",
       "       6.6467110e+01, 2.9150665e+02, 2.1287351e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 5.1694221e+00, 8.6347619e-03, 0.0000000e+00,\n",
       "       0.0000000e+00, 1.3868414e+02, 7.9492259e+00, 3.0789349e+01,\n",
       "       2.4015063e+02, 2.1194975e+00, 2.1530846e+01, 0.0000000e+00,\n",
       "       1.5235422e+00, 0.0000000e+00, 1.1791686e+01, 1.5501668e+02,\n",
       "       1.1664439e+02, 0.0000000e+00, 0.0000000e+00, 9.9615359e-01,\n",
       "       9.1794186e+00, 4.3738323e+01, 1.1915057e+02, 1.2593509e+00,\n",
       "       3.0318619e-04, 9.3776321e-01, 0.0000000e+00, 8.5476913e+00,\n",
       "       1.6754950e+01, 8.7193909e+01, 0.0000000e+00, 4.0190002e+01,\n",
       "       0.0000000e+00, 6.2034407e+00], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array(activations['relu_3_2']).sum(axis=2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 200)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_activation = np.array(activations['out_3']).mean(axis=0)\n",
    "final_activation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## only to verify how the bias and linear weights are applied to the final resBlock output\n",
    "#\n",
    "#manual_result = []\n",
    "#for time_step in range(200):\n",
    "#    time_step_result = 0\n",
    "#    for channel in range(50):\n",
    "#        time_step_result += final_activation[channel, time_step] * model.linear.weight[0, channel].item()\n",
    "#    time_step_result += model.linear.bias.item()\n",
    "#    manual_result.append(time_step_result)\n",
    "#manual_result = np.array(manual_result)\n",
    "#true_result = model.linear(torch.Tensor(final_activation).view((1,50,200)).transpose(1,2)).transpose(1,2)[0, 0, :].detach().numpy()\n",
    "#print(manual_result, \"\\n\", true_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_activation_mean = final_activation.sum(axis=1)\n",
    "final_activation_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-25.791216 , -29.770973 ,   6.339761 ,   4.9586186, -12.819422 ,\n",
       "         2.202875 ,   4.037536 , -17.158644 ,   6.8134437,   6.012519 ,\n",
       "        -3.4208786,   7.0462146,   8.65969  , -19.969051 ,   2.9791844,\n",
       "         3.0739796,   3.485335 ,   5.4982734,   6.2807584,   4.5516543,\n",
       "         4.680796 , -17.39781  ,   4.780748 ,   6.6658893, -21.025614 ,\n",
       "         5.6688247,   6.7976456,   0.2760217,   6.1693234,   5.441935 ,\n",
       "         6.8244224, -10.218779 , -17.764706 ,   2.8205185,   0.8942052,\n",
       "         4.26103  ,   5.4359984,   6.8160124, -29.810795 ,   5.1186614,\n",
       "       -16.313679 ,   5.8977733,   2.509643 ,   5.0556064,   7.614464 ,\n",
       "       -27.394215 ,   4.817239 ,   7.8838835,   5.5701647,   6.2224965],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_activation_mean_scaled = final_activation_mean * model.linear.weight[0].detach().numpy()\n",
    "final_activation_mean_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_activation_means_df = pd.DataFrame()\n",
    "final_activation_means_df['mean_out_3'] = final_activation_mean\n",
    "final_activation_means_df['mean_out_3_scaled'] = np.abs(final_activation_mean_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 11})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_activation_means_df_sorted_out3 = final_activation_means_df.sort_values('mean_out_3', axis=0, ascending=False)\n",
    "labels = final_activation_means_df_sorted_out3.index.values\n",
    "x = np.arange(final_activation_means_df_sorted_out3['mean_out_3'].values.shape[0])\n",
    "y = final_activation_means_df_sorted_out3['mean_out_3'].values\n",
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "ax.plot(y)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_xlabel('Channel', fontsize=18*0.8)\n",
    "ax.set_ylabel('Activation', fontsize=18*0.8)\n",
    "ax.set_title('Channel Activation after Residual Block 3')\n",
    "plt.tight_layout()\n",
    "#fig.savefig(os.path.join(experiment_dir, 'channel_activation_distribution_out_3.svg'), format='svg')\n",
    "fig.savefig(os.path.join(experiment_dir, 'channel_activation_distribution_out_3.pgf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_activation_means_df_sorted_out3_scaled = final_activation_means_df.sort_values('mean_out_3_scaled', axis=0, ascending=False)\n",
    "labels = final_activation_means_df_sorted_out3_scaled.index.values\n",
    "x = np.arange(final_activation_means_df_sorted_out3_scaled['mean_out_3_scaled'].values.shape[0])\n",
    "y = final_activation_means_df_sorted_out3_scaled['mean_out_3_scaled'].values\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "ax.plot(y)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_xlabel('Channel', fontsize=18*0.8)\n",
    "ax.set_ylabel('Activation', fontsize=18*0.8)\n",
    "#ax.set_title('Channel Activation after the Linear Layer')\n",
    "plt.tight_layout()\n",
    "#fig.savefig(os.path.join(experiment_dir, 'channel_activation_distribution_out_linear.svg'), format='svg')\n",
    "fig.savefig(os.path.join(experiment_dir, 'channel_activation_distribution_out_linear.pgf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Activation_Visualization/Experiments/2019-07-19 15-01-48.156239__A__final/'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
